---
layout: page
title: [2021-11-17]-ruibin-report
permalink: /static/ruibin/progress/[2021-11-17]-ruibin-report/
---

[**<-back**](/static/ruibin/progress)  

## What did I do

1. planing the sturcture and directions of my final thesis.

2. collecting papers on "knowledge graph embedding" and "enhanced pre-training model with knowledge graph", then focus on the method of combining (also can see as an embedding) pre-trained model and knowledge graph.

3. Follow the tutorial of [[Knowledge Graph Embeddings: From Theory to Practice]](https://colab.research.google.com/drive/1Fcf8vkuaO6VCOB3MAZlpDebCAgyUnMBj?usp=sharing#scrollTo=pdX1lwK4rX_Y), understand how they are training, optimizing and gradient decrease.  


## what I will do next

focus on the part of combining KG and pre-trained model, and make some breakthrough.



## Reference

[1] Logan IV, Robert L., et al. "Barack's wife hillary: Using knowledge-graphs for fact-aware language modeling." arXiv preprint arXiv:1906.07241 (2019).

[2] BERT-MK_Integrating Graph Contextualized Knowledge into Pre-trained Language Models

[3] Sun, Tianxiang, et al. "Colake: Contextualized language and knowledge embedding." arXiv preprint arXiv:2010.00309 (2020).

[4] Sun, Yu, et al. "Ernie 2.0: A continual pre-training framework for language understanding." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 05. 2020.

[5] Zhang, Zhengyan, et al. "ERNIE: Enhanced language representation with informative entities." arXiv preprint arXiv:1905.07129 (2019).

[6] Zhang, Zhengyan, et al. "ERNIE: Enhanced language representation with informative entities." arXiv preprint arXiv:1905.07129 (2019).

[7] Shen, Tao, et al. "Exploiting structured knowledge in text via graph-guided representation learning." arXiv preprint arXiv:2004.14224 (2020).

[8] Liu, Weijie, et al. "K-bert: Enabling language representation with knowledge graph." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 03. 2020.

[9] Wang, Xiaozhi, et al. "KEPLER: A unified model for knowledge embedding and pre-trained language representation." Transactions of the Association for Computational Linguistics 9 (2021): 176-194.


